{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f23f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (30, 10)\n",
    "plt.rcParams['font.size']=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e1258",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "- Can be used to \"plan\" when MDP $(S, P, d, R, A)$ is known\n",
    "\n",
    "#### 1) Policy Evaluation / Prediction\n",
    "\n",
    "- Given a MDP $(S, P, d, R, A)$ for any arbitrary policy $\\pi$ we can find $V^\\pi$\n",
    "- Guess $V^\\pi_{0}$, and calculate $V^\\pi_{1}$, $V^\\pi_{2}$, ... \n",
    "- for each state $s$, $V^\\pi_{k+1}(s) = \\sum_{a} \\pi(a|s) \\left( R^a_{s} + \\sum_{s'} P^a_{s,s'} V^\\pi_{k}(s')\\right)$\n",
    "- Contraction mapping theorem says that RHS is a contraction map and will converge on repeated iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d23315ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MDP\n",
    "S = np.array([0, 2]) \n",
    "A = np.array([100, 200])\n",
    "P = np.array([[[0.5, 0.5],   # P given a=0\n",
    "              [0.8, 0.2]], \n",
    "     \n",
    "             [[0.2, 0.8],   # P given a=0\n",
    "              [0.4, 0.6]]] \n",
    " \n",
    "             )\n",
    "R = np.array([[10, 90], \n",
    "               [5, 30]])\n",
    "d = 0.95\n",
    "s0 = 0\n",
    "pi = np.array([[0.5, 0.5], \n",
    "               [0.5, 0.5]])\n",
    "T = 100\n",
    "\n",
    "def discountedSum(R, d):\n",
    "    sum = 0\n",
    "    for i, r in enumerate(R):\n",
    "        sum += R[i] * (d ** i)\n",
    "    return sum\n",
    "\n",
    "# Histories\n",
    "HS = np.zeros(T+1).astype(int)\n",
    "HR = np.zeros(T).astype(int)\n",
    "HA = np.zeros(T).astype(int)\n",
    "HG = np.zeros(T).astype(int)\n",
    "HV = np.zeros(T).astype(int)\n",
    "HS[0] = s0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e99f95",
   "metadata": {},
   "source": [
    "* Value Function Iteration for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af2e5858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n",
      "[673.93939394 716.36363636]\n"
     ]
    }
   ],
   "source": [
    "N = 1000 # iterations\n",
    "V_new = np.zeros(S.shape[0])\n",
    "V_old = np.ones(S.shape[0])\n",
    "print(V_old)\n",
    "for i_n in range(N):\n",
    "    for i_s, s in enumerate(S):\n",
    "        V_new[i_s] = np.dot(pi[i_s], R[:, i_s] + d * np.dot(P[:, i_s, :], V_old))\n",
    "    V_old = V_new.copy()\n",
    "print(V_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b1df2",
   "metadata": {},
   "source": [
    "* Monte Carlo Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8a140bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[672.09  706.266]\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo\n",
    "N = 500 # histories\n",
    "V = np.empty((T, S.shape[0]))\n",
    "Q = np.empty((T, A.shape[0], S.shape[0]))\n",
    "G_ = np.empty((T, S.shape[0], N))\n",
    "for i_s, s0 in enumerate(S):\n",
    "    for i_n in range(N):\n",
    "        HS = np.zeros(T+1).astype(int)\n",
    "        HR = np.zeros(T).astype(int)\n",
    "        HA = np.zeros(T).astype(int)\n",
    "        HG = np.zeros(T).astype(int)\n",
    "        HV = np.zeros(T).astype(int)\n",
    "        HS[0] = s0\n",
    "        for t in range(T):\n",
    "            HA[t] = np.random.choice(A, p = pi[np.where(S == HS[t])][0])\n",
    "            HS[t+1] = np.random.choice(S, p = P[np.where(A == HA[t]), np.where(S == HS[t]), :][0][0])\n",
    "            HR[t] = R[np.where(A == HA[t]), np.where(S == HS[t])]\n",
    "        for t in range(0, T, 1):\n",
    "            HG[T-1-t] = discountedSum(HR[T-1-t:T], d)\n",
    "        G_[:, i_s, i_n] = HG\n",
    "    for t in range(T-1):\n",
    "        V[t, i_s] = np.mean(G_[t, i_s, :])           \n",
    "print(V[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0044fa",
   "metadata": {},
   "source": [
    "#### 2) Optimal Policy / Control\n",
    "\n",
    "- Given a MDP $(S, P, d, R, A)$ we can find $V^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c111f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "V[0, i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
